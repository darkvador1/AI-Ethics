{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chemin de l'exécutable Python : c:\\Users\\micha\\AppData\\Local\\Programs\\Python\\Python311\\python.exe\n",
      "Version de Python : 3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Chemin de l'exécutable Python :\", sys.executable)\n",
    "print(\"Version de Python :\", sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Load all necessary packages\n",
    "# import sys\n",
    "# sys.path.append(\"../\")\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.datasets import AdultDataset, GermanDataset, CompasDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from aif360.algorithms.preprocessing.reweighing import Reweighing\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions\\\n",
    "        import load_preproc_data_adult, load_preproc_data_german, load_preproc_data_compas\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from IPython.display import Markdown, display\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. - Preprocess the data\n",
    "- binarize Age. \n",
    "- Split the data into train, validation, test sets\n",
    "- train a classifier\n",
    "- Measure the performance of the classifier on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Missing Data: 3620 rows removed from AdultDataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set scores (first 10 predictions):\n",
      "[[0.05110014]\n",
      " [0.01196516]\n",
      " [0.05237648]\n",
      " [0.64457542]\n",
      " [0.61894245]\n",
      " [0.48253575]\n",
      " [0.01588735]\n",
      " [0.00462869]\n",
      " [0.00106118]\n",
      " [0.26590193]]\n",
      "Test Accuracy: 0.8438974056603774\n",
      "Test Precision: 0.7225392296718973\n",
      "Test Recall: 0.6019013666072489\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random_seed = 42\n",
    "random.seed(random_seed)  \n",
    "np.random.seed(random_seed)  \n",
    "# A -------------------------------- Import dataaset and binarize age ----------------------------------------------\n",
    "\n",
    "# Import dataset\n",
    "dataset_orig = AdultDataset()\n",
    "\n",
    "# Convert to dataframe to visualize it\n",
    "data, _ = dataset_orig.convert_to_dataframe()\n",
    "\n",
    "# Copy only to binarize age (Threshold = 40 years)\n",
    "binarised = data.copy()\n",
    "binarised['age'] = binarised['age'].apply(lambda x: 0 if x <= 40 else 1) # 1 if greater than 40\n",
    "\n",
    "# B ----------------------------- Split into train, validation and test --------------------------------------------\n",
    "\n",
    "# Separate into train (70%) and validation+test (30%)\n",
    "\n",
    "dataset_orig_train, dataset_orig_vt = dataset_orig.split([0.7], shuffle=True)\n",
    "# Separate into validation and test\n",
    "\n",
    "dataset_orig_valid, dataset_orig_test = dataset_orig_vt.split([0.5], shuffle=True)\n",
    "# C ----------------------------- Train a classifier --------------------------------------------\n",
    "\n",
    "# Define first the target and our features (Target is income-per-year (1 --> >50 k))\n",
    "\n",
    "# 1. Normalise training data\n",
    "scale_orig = StandardScaler()\n",
    "X_train = scale_orig.fit_transform(dataset_orig_train.features)  # Features\n",
    "y_train = dataset_orig_train.labels.ravel()  # Target (income-per-year)\n",
    "w_train = dataset_orig_train.instance_weights.ravel()  \n",
    "\n",
    "# 2. Train the classifier\n",
    "the_classifier = LogisticRegression()\n",
    "the_classifier.fit(X_train, y_train, sample_weight=w_train) \n",
    "\n",
    "# 3. Predictions\n",
    "y_train_pred = the_classifier.predict(X_train)  # Predict labels on training set\n",
    "\n",
    "# Positive class (Here, >50K)\n",
    "pos_ind = np.where(the_classifier.classes_ == dataset_orig_train.favorable_label)[0][0] \n",
    "\n",
    "dataset_orig_train_pred = dataset_orig_train.copy()\n",
    "dataset_orig_train_pred.labels = y_train_pred  # Replace labels by the predictions\n",
    "\n",
    "# D ----------------------------- Performances on the test set --------------------------------------------\n",
    "dataset_orig_test_pred = dataset_orig_test.copy(deepcopy=True)\n",
    "\n",
    "# Normalise as in the training and extract labels of test set\n",
    "X_test = scale_orig.transform(dataset_orig_test_pred.features)\n",
    "y_test = dataset_orig_test_pred.labels\n",
    "\n",
    "# Predict probability for class\n",
    "dataset_orig_test_pred.scores = the_classifier.predict_proba(X_test)[:, pos_ind].reshape(-1, 1)\n",
    "\n",
    "print(\"Test set scores (first 10 predictions):\")\n",
    "print(dataset_orig_test_pred.scores[:10])\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "# Convert scores to binary predictions (threshold = 0.5)\n",
    "y_test_pred = (dataset_orig_test_pred.scores > 0.5).astype(int)\n",
    "\n",
    "# Compute main metrics\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_test_pred)}\")\n",
    "print(f\"Test Precision: {precision_score(y_test, y_test_pred)}\")\n",
    "print(f\"Test Recall: {recall_score(y_test, y_test_pred)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. - Assess the group fairness of the classifier, assuming the protected attributes are Age, Sex. \n",
    "- apply a technique to ensure the classifier is fair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instance weights before reweighting:\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Test set fairness metrics (classifier):"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical Parity Difference (classifier): -0.17871986305566923\n",
      "\n",
      "Instance weights after reweighting:\n",
      "[0.8457202  1.09594426 0.78982139 1.09594426 1.09594426 0.78982139\n",
      " 0.78982139 1.09594426 0.8457202  1.09594426]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Transformed training dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups (Sex):\n",
      "5.551115123125783e-17\n",
      "\n",
      "Test set scores (first 10 predictions) for the fair classifier:\n",
      "[[0.0344237 ]\n",
      " [0.00917653]\n",
      " [0.12897345]\n",
      " [0.56631903]\n",
      " [0.54080346]\n",
      " [0.41756688]\n",
      " [0.03856527]\n",
      " [0.00350017]\n",
      " [0.0005973 ]\n",
      " [0.20684067]]\n",
      "Main classifier :\n",
      "\n",
      "Test Accuracy: 0.8438974056603774\n",
      "Test Precision: 0.7225392296718973\n",
      "Test Recall: 0.6019013666072489\n",
      "\n",
      "Fair classifier :\n",
      "\n",
      "Fair Classifier - Test Accuracy: 0.8365271226415094\n",
      "Fair Classifier - Test Precision: 0.7235202492211839\n",
      "Fair Classifier - Test Recall: 0.5519904931669638\n"
     ]
    }
   ],
   "source": [
    "random_seed = 42\n",
    "random.seed(random_seed) \n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# A -------- Assess the group fairness of the classifier, assuming the protected attributes are Age, Sex ------------ \n",
    "\n",
    "privileged_groups = [{'sex': 1}]  # Men are priviledged\n",
    "unprivileged_groups = [{'sex': 0}]  \n",
    "\n",
    "test_predicted = dataset_orig_test.copy()\n",
    "test_predicted.labels = y_test_pred  # Prédictions du classifier\n",
    "\n",
    "print(\"\\nInstance weights before reweighting:\")\n",
    "print(test_predicted.instance_weights[:10])  \n",
    "# Compute fairness of the classifier (test set)\n",
    "metric_test_classifier = BinaryLabelDatasetMetric(test_predicted, \n",
    "                                                  unprivileged_groups=unprivileged_groups,\n",
    "                                                  privileged_groups=privileged_groups)\n",
    "\n",
    "display(Markdown(\"#### Test set fairness metrics (classifier):\"))\n",
    "print(\"Statistical Parity Difference (classifier):\", metric_test_classifier.statistical_parity_difference())\n",
    "\n",
    "# B ---------------- apply a technique to ensure the classifier is fair  ----------------------------------------------\n",
    "\n",
    "RW = Reweighing(unprivileged_groups=unprivileged_groups, \n",
    "                privileged_groups=privileged_groups)\n",
    "dataset_train_transf = RW.fit_transform(dataset_orig_train)\n",
    "\n",
    "print(\"\\nInstance weights after reweighting:\")\n",
    "print(dataset_train_transf.instance_weights[:10])  \n",
    "\n",
    "metric_transf_train = BinaryLabelDatasetMetric(dataset_train_transf, \n",
    "                                               unprivileged_groups=unprivileged_groups,\n",
    "                                               privileged_groups=privileged_groups)\n",
    "print(\"\\n\")\n",
    "display(Markdown(\"#### Transformed training dataset\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups (Sex):\")\n",
    "print(metric_transf_train.statistical_parity_difference())\n",
    "\n",
    "# C ------------------------- Train our 'fair classifier' on the new dataset ----------------------------------------------\n",
    "# Normalise transformed data\n",
    "X_train_rw = scale_orig.fit_transform(dataset_train_transf.features)  # Features corrigées\n",
    "y_train_rw = dataset_train_transf.labels.ravel()  # Labels corrigés\n",
    "w_train_rw = dataset_train_transf.instance_weights.ravel()  # Poids ajustés\n",
    "\n",
    "# Train model again with transformed data\n",
    "fair_classifier = LogisticRegression(random_state=random_seed)\n",
    "fair_classifier.fit(X_train_rw, y_train_rw, sample_weight=w_train_rw)\n",
    "\n",
    "# Predictions \n",
    "y_train_pred_fair = fair_classifier.predict(X_train_rw)\n",
    "\n",
    "dataset_train_transf_pred = dataset_train_transf.copy()\n",
    "dataset_train_transf_pred.labels = y_train_pred_fair\n",
    "\n",
    "# D ------------------------------- Performances on test set (Fair classifier) --------------------------------------\n",
    "X_test_rw = scale_orig.transform(dataset_orig_test.features)\n",
    "y_test = dataset_orig_test.labels\n",
    "\n",
    "# Predictions with the fair classifier\n",
    "dataset_orig_test_pred_fair = dataset_orig_test.copy()\n",
    "y_test_pred_fair = fair_classifier.predict(X_test_rw)\n",
    "\n",
    "dataset_orig_test_pred_fair.scores = fair_classifier.predict_proba(X_test_rw)[:, pos_ind].reshape(-1, 1)\n",
    "\n",
    "# Display scores for 10 first instances\n",
    "print(\"\\nTest set scores (first 10 predictions) for the fair classifier:\")\n",
    "print(dataset_orig_test_pred_fair.scores[:10])\n",
    "\n",
    "y_test_pred_fair_bin = (dataset_orig_test_pred_fair.scores > 0.5).astype(int)\n",
    "\n",
    "# Main metrics\n",
    "print(\"Main classifier :\\n\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_test_pred)}\")\n",
    "print(f\"Test Precision: {precision_score(y_test, y_test_pred)}\")\n",
    "print(f\"Test Recall: {recall_score(y_test, y_test_pred)}\")\n",
    "print(\"\\nFair classifier :\\n\")\n",
    "print(f\"Fair Classifier - Test Accuracy: {accuracy_score(y_test, y_test_pred_fair_bin)}\")\n",
    "print(f\"Fair Classifier - Test Precision: {precision_score(y_test, y_test_pred_fair_bin)}\")\n",
    "print(f\"Fair Classifier - Test Recall: {recall_score(y_test, y_test_pred_fair_bin)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
